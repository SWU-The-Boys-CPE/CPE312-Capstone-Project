# Bangkok Traffic Flow Optimization - T3: Modeling, Analysis & Evaluation

**Phase 3: Model Development, Analysis, and Performance Evaluation**

**Duration:** 2 weeks (Week 5-6)  
**Status:** ğŸ”œ In Progress  
**Last Updated:** November 27, 2025

---

## ğŸ“‹ Quick Navigation

| Section | Purpose | Go To |
|---------|---------|-------|
| **Getting Started** | New to this phase? | â†“ Below |
| **Project Overview** | What is T3? | [Project Overview](#-project-overview) |
| **Directory Structure** | Where are things? | [Directory Structure](#-directory-structure) |
| **Key Documents** | What should I read? | [Key Documents](#-key-documents) |
| **Your Role** | What do I need to do? | [Your Role](#-your-role) |
| **How to Use This** | Code & scripts | [How to Use](#-how-to-use-this-phase) |
| **Deliverables** | What needs to be done? | [Deliverables](#-deliverables) |
| **Daily Tasks** | Today's work | [Week 3 Checklist](Week_3_Modeling_Analysis_Evaluation_Checklist_Template.md) |

---

## ğŸ¯ Project Overview

### What is T3?

**T3** is Phase 3 of the Bangkok Traffic Flow Optimization capstone project. This phase focuses on:

âœ… **Model Selection** - Choosing appropriate ML/statistical models for traffic prediction  
âœ… **Model Development** - Training, tuning, and optimizing selected models  
âœ… **Feature Engineering** - Creating advanced features for model training  
âœ… **Model Evaluation** - Rigorous performance testing and validation  
âœ… **Analysis & Interpretation** - Deriving insights from model results  
âœ… **Documentation** - Recording methodology, findings, and limitations  

### Research Questions Addressed

1. **Traffic Pattern Prediction:** Can we predict congestion levels 15-60 minutes in advance with 75-85% accuracy?
2. **Spatial Hotspot Identification:** What are the most congested areas and why?
3. **Weather-Traffic Correlation:** How do weather conditions affect traffic patterns?
4. **Feature Importance:** Which features are most predictive of congestion?

### T3 Deliverables

By the end of Phase 3, we will have:

| # | Deliverable | Description | Owner |
|---|-------------|-------------|-------|
| 1 | **Trained Models** | At least 3 models (LSTM, XGBoost, ARIMA) trained and validated | Data Scientist |
| 2 | **Model Selection Report** | Justification for model choices based on data and research questions | Data Scientist |
| 3 | **Evaluation Report** | Performance metrics, validation results, cross-validation | Data Scientist |
| 4 | **Feature Importance Analysis** | Ranking of predictive features with visualizations | Data Analyst |
| 5 | **Analysis Findings** | Interpretation of results, alignment with hypotheses | Team |
| 6 | **Limitations Documentation** | Known constraints, biases, and areas for improvement | Tech Lead |

---

## ğŸ“‚ Directory Structure

```
Worked/T3/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                                 â† You are here
â”œâ”€â”€ ğŸ“„ Week_3_Modeling_Analysis_Evaluation_Checklist_Template.md
â”‚
â”œâ”€â”€ ğŸ“‚ 01_Model_Selection/
â”‚   â”œâ”€â”€ ğŸ“„ README.md                            â­â­â­ Model selection overview
â”‚   â”œâ”€â”€ ğŸ“„ Model_Selection_Justification.md     â­â­â­ Why these models?
â”‚   â””â”€â”€ ğŸ“„ Model_Comparison_Matrix.md           Model features comparison
â”‚
â”œâ”€â”€ ğŸ“‚ 02_Model_Development/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“‚ Trained_Models/                      â† Saved model artifacts
â”‚   â”‚   â”œâ”€â”€ lstm_traffic_model.h5
â”‚   â”‚   â”œâ”€â”€ xgboost_traffic_model.pkl
â”‚   â”‚   â””â”€â”€ arima_traffic_model.pkl
â”‚   â”œâ”€â”€ ğŸ“‚ Hyperparameter_Tuning/
â”‚   â”‚   â”œâ”€â”€ tuning_results.csv
â”‚   â”‚   â””â”€â”€ best_params.yaml
â”‚   â””â”€â”€ ğŸ“‚ Model_Checkpoints/
â”‚
â”œâ”€â”€ ğŸ“‚ 03_Model_Evaluation/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ Evaluation_Metrics.md                â­â­â­ Performance metrics
â”‚   â”œâ”€â”€ ğŸ“„ Validation_Results.md                Cross-validation results
â”‚   â”œâ”€â”€ ğŸ“„ Limitations.md                       Known constraints
â”‚   â””â”€â”€ ğŸ“‚ Evaluation_Reports/
â”‚       â”œâ”€â”€ model_comparison_report.md
â”‚       â””â”€â”€ cross_validation_report.md
â”‚
â”œâ”€â”€ ğŸ“‚ 04_Analysis/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ Analysis_Findings.md                 â­â­â­ Key analysis results
â”‚   â”œâ”€â”€ ğŸ“„ Feature_Importance.md                Feature ranking analysis
â”‚   â”œâ”€â”€ ğŸ“„ Hypothesis_Validation.md             Research question answers
â”‚   â””â”€â”€ ğŸ“„ Key_Insights.md                      â­â­â­ Summary of insights
â”‚
â”œâ”€â”€ ğŸ“‚ 05_Scripts/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ modeling.py                          Model training functions
â”‚   â”œâ”€â”€ ğŸ evaluation.py                        Evaluation metrics & validation
â”‚   â”œâ”€â”€ ğŸ model_utils.py                       Helper functions for models
â”‚   â”œâ”€â”€ ğŸ feature_engineering.py               Advanced feature creation
â”‚   â””â”€â”€ ğŸ“‚ __pycache__/
â”‚
â”œâ”€â”€ ğŸ“‚ 06_Notebooks/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ““ 04_Feature_Engineering.ipynb         â­â­â­ Create model features
â”‚   â”œâ”€â”€ ğŸ““ 05_Model_Training.ipynb              â­â­â­ Train all models
â”‚   â”œâ”€â”€ ğŸ““ 06_Model_Evaluation.ipynb            â­â­â­ Evaluate performance
â”‚   â””â”€â”€ ğŸ““ 07_Model_Interpretation.ipynb        Feature importance & insights
â”‚
â”œâ”€â”€ ğŸ“‚ 07_Documentation/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“‚ 01_Methodology/
â”‚   â”‚   â””â”€â”€ Modeling_Methodology.md             â­â­â­ How we model
â”‚   â”œâ”€â”€ ğŸ“‚ 02_Technical_Reference/
â”‚   â”‚   â”œâ”€â”€ Model_Architecture.md               Model specifications
â”‚   â”‚   â””â”€â”€ Hyperparameter_Reference.md         Parameter documentation
â”‚   â”œâ”€â”€ ğŸ“‚ 03_Progress_Reports/
â”‚   â”‚   â”œâ”€â”€ Week05_Progress.md                  Weekly progress
â”‚   â”‚   â””â”€â”€ Week06_Progress.md
â”‚   â””â”€â”€ ğŸ“‚ 04_Quality_Assurance/
â”‚       â”œâ”€â”€ Model_QA_Checklist.md               Quality checks
â”‚       â””â”€â”€ Reproducibility_Guide.md            How to reproduce
â”‚
â”œâ”€â”€ ğŸ“‚ 08_Configuration/
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ model_config.yaml                    â­â­â­ Model configurations
â”‚   â””â”€â”€ ğŸ“„ requirements.txt                     Python dependencies
â”‚
â””â”€â”€ ğŸ“‚ 09_Results/
    â”œâ”€â”€ ğŸ“„ README.md
    â”œâ”€â”€ ğŸ“‚ Figures/                             â† Model visualizations
    â”‚   â”œâ”€â”€ model_comparison.png
    â”‚   â”œâ”€â”€ learning_curves.png
    â”‚   â”œâ”€â”€ feature_importance.png
    â”‚   â””â”€â”€ confusion_matrices.png
    â”œâ”€â”€ ğŸ“‚ Predictions/
    â”‚   â””â”€â”€ test_predictions.csv
    â””â”€â”€ ğŸ“‚ Reports/
        â””â”€â”€ final_model_report.pdf
```

**Key Locations:**
- ğŸ¤– **Save Models:** `02_Model_Development/Trained_Models/`
- ğŸ“Š **Save Metrics:** `03_Model_Evaluation/Evaluation_Reports/`
- ğŸ’» **Use Code:** `05_Scripts/`
- ğŸ““ **Run Notebooks:** `06_Notebooks/`
- ğŸ“ˆ **Save Figures:** `09_Results/Figures/`

---

## ğŸ“š Key Documents

### Essential Reading (Required)

| Document | Purpose | Read Time | Priority |
|----------|---------|-----------|----------|
| [Model_Selection_Justification.md](01_Model_Selection/Model_Selection_Justification.md) | Why we chose these models | 15 min | ğŸ”´ URGENT |
| [Modeling_Methodology.md](07_Documentation/01_Methodology/Modeling_Methodology.md) | How we approach modeling | 20 min | ğŸ”´ URGENT |
| [Evaluation_Metrics.md](03_Model_Evaluation/Evaluation_Metrics.md) | How we measure performance | 10 min | â­â­â­ |
| [model_config.yaml](08_Configuration/model_config.yaml) | Model configurations | 10 min | â­â­â­ |

### Reference Documents

| Document | Purpose | Use When |
|----------|---------|----------|
| [Feature_Importance.md](04_Analysis/Feature_Importance.md) | Feature rankings | Interpreting results |
| [Limitations.md](03_Model_Evaluation/Limitations.md) | Known constraints | Writing conclusions |
| [Hyperparameter_Reference.md](07_Documentation/02_Technical_Reference/Hyperparameter_Reference.md) | Parameter details | Tuning models |

---

## ğŸ‘¥ Your Role

### If You're the Data Scientist (à¸„à¸²à¸¡à¸´à¸™ à¸ªà¸¸à¸£à¸‚à¸ˆà¸£)

**Your Focus:** Model Development, Training, Validation

**T3 Tasks:**
1. âœ… Select appropriate models based on research questions
2. âœ… Implement feature engineering pipeline
3. âœ… Train LSTM, XGBoost, and ARIMA models
4. âœ… Perform hyperparameter tuning
5. âœ… Cross-validate model performance
6. âœ… Document model architecture and decisions
7. âœ… Create model interpretation notebook

**Key Files to Use:**
- `05_Scripts/modeling.py` - Model training
- `05_Scripts/evaluation.py` - Performance metrics
- `06_Notebooks/05_Model_Training.ipynb` - Training workflow
- `08_Configuration/model_config.yaml` - Configurations

**Deliverables:**
- Trained models in `02_Model_Development/Trained_Models/`
- `Model_Selection_Justification.md`
- `06_Model_Evaluation.ipynb` notebook

---

### If You're the Data Analyst (à¸§à¸µà¸£à¹Œà¸à¸§à¸´à¸™ à¸™à¸²à¸„à¸™à¸´à¸˜à¸´à¸Šà¸±à¸¢à¸£à¸±à¸Šà¸•à¹Œ)

**Your Focus:** Feature Engineering, Analysis, Visualization

**T3 Tasks:**
1. âœ… Prepare data for modeling (from T2 cleaned data)
2. âœ… Create advanced features (lag, rolling, interactions)
3. âœ… Analyze feature importance
4. âœ… Create visualizations for model results
5. âœ… Document analysis findings
6. âœ… Interpret results in context of research questions

**Key Files to Use:**
- `05_Scripts/feature_engineering.py` - Feature creation
- `06_Notebooks/04_Feature_Engineering.ipynb` - Feature workflow
- T2 cleaned data: `../T2/02_Data/Processed/`

**Deliverables:**
- `04_Feature_Engineering.ipynb` notebook
- Feature importance visualizations
- `Analysis_Findings.md`

---

### If You're the Technical Lead (à¸à¸¤à¸•à¸ à¸²à¸ª à¸­à¸´à¹ˆà¸¡à¸—à¸±à¹ˆà¸§)

**Your Focus:** Code Quality, Reproducibility, QA

**T3 Tasks:**
1. âœ… Code review for all modeling scripts
2. âœ… Ensure reproducibility (random seeds, versioning)
3. âœ… QA testing for model pipeline
4. âœ… Validate model performance claims
5. âœ… Document limitations and constraints
6. âœ… Create reproducibility guide

**Key Files to Use:**
- `05_Scripts/` - All Python modules
- `07_Documentation/04_Quality_Assurance/` - QA documents
- `08_Configuration/` - Configuration files

**Deliverables:**
- `Reproducibility_Guide.md`
- `Model_QA_Checklist.md`
- Code review feedback

---

### If You're the Project Manager (à¸™à¸´à¸•à¸´à¸ à¸¹à¸¡à¸´ à¹‚à¸à¸˜à¸´à¸Šà¸±à¸¢)

**Your Focus:** Coordination, Progress Tracking, Synthesis

**T3 Tasks:**
1. âœ… Track modeling progress
2. âœ… Coordinate between team members
3. âœ… Ensure deliverables on schedule
4. âœ… Synthesize key insights
5. âœ… Update status reports
6. âœ… Prepare for T4 presentation phase

**Key Files to Use:**
- `Week_3_Modeling_Analysis_Evaluation_Checklist_Template.md` - Checklist
- `07_Documentation/03_Progress_Reports/` - Weekly reports
- `04_Analysis/Key_Insights.md` - Summary

**Deliverables:**
- Weekly progress reports
- `Key_Insights.md` summary
- Team coordination

---

## ğŸ› ï¸ How to Use This Phase

### Step 1: Prepare Data for Modeling

**Prerequisites:** Complete T2 (cleaned data available)

```python
# Load cleaned data from T2
import pandas as pd
from pathlib import Path

data_path = Path('../T2/02_Data/Processed')
df_traffic = pd.read_csv(data_path / 'bangkok_traffic_cleaned.csv')
df_weather = pd.read_csv(data_path / 'bangkok_weather_cleaned.csv')

print(f"Traffic data: {df_traffic.shape}")
print(f"Weather data: {df_weather.shape}")
```

---

### Step 2: Feature Engineering

**Location:** `06_Notebooks/04_Feature_Engineering.ipynb`

```python
from scripts.feature_engineering import create_model_features

# Create features for modeling
df_features = create_model_features(
    df_traffic,
    target_col='congestion_index',
    lag_periods=[1, 7, 14, 30],
    rolling_windows=[7, 14, 30]
)

print(f"Features created: {df_features.shape[1]} columns")
```

---

### Step 3: Train Models

**Location:** `06_Notebooks/05_Model_Training.ipynb`

```python
from scripts.modeling import (
    train_lstm_model,
    train_xgboost_model,
    train_arima_model
)
from scripts.evaluation import evaluate_model

# Train models
lstm_model = train_lstm_model(X_train, y_train, config)
xgboost_model = train_xgboost_model(X_train, y_train, config)
arima_model = train_arima_model(y_train, config)

# Evaluate
lstm_metrics = evaluate_model(lstm_model, X_test, y_test)
print(f"LSTM RMSE: {lstm_metrics['rmse']:.4f}")
```

---

### Step 4: Evaluate and Compare

**Location:** `06_Notebooks/06_Model_Evaluation.ipynb`

```python
from scripts.evaluation import (
    calculate_all_metrics,
    cross_validate_model,
    create_comparison_report
)

# Cross-validation
cv_results = cross_validate_model(model, X, y, cv=5)

# Compare all models
comparison = create_comparison_report([lstm_model, xgboost_model, arima_model])
print(comparison)
```

---

### Step 5: Document Findings

**Update these documents:**
1. `04_Analysis/Analysis_Findings.md` - Results interpretation
2. `04_Analysis/Key_Insights.md` - Summary of insights
3. `03_Model_Evaluation/Limitations.md` - Known constraints

---

## ğŸ“‹ Deliverables

### Phase 3 Deliverables Checklist

**Model Deliverables:**
- [ ] LSTM model trained and saved
- [ ] XGBoost model trained and saved
- [ ] ARIMA model trained and saved
- [ ] Hyperparameter tuning completed
- [ ] Model checkpoints saved

**Evaluation Deliverables:**
- [ ] Evaluation metrics documented (MAE, RMSE, MAPE, RÂ²)
- [ ] Cross-validation results
- [ ] Model comparison report
- [ ] Confusion matrices (if applicable)
- [ ] Learning curves

**Analysis Deliverables:**
- [ ] Feature importance analysis
- [ ] Hypothesis validation
- [ ] Key insights documented
- [ ] Limitations identified

**Code Deliverables:**
- [ ] `04_Feature_Engineering.ipynb` - Feature creation
- [ ] `05_Model_Training.ipynb` - Model training
- [ ] `06_Model_Evaluation.ipynb` - Performance evaluation
- [ ] `07_Model_Interpretation.ipynb` - Insights
- [ ] All notebooks reproducible and documented

**Documentation Deliverables:**
- [ ] `Model_Selection_Justification.md`
- [ ] `Evaluation_Metrics.md`
- [ ] `Limitations.md`
- [ ] `Key_Insights.md`

---

## ğŸ“Š Model Selection Summary

### Models Selected for Traffic Prediction

| Model | Type | Use Case | Strengths |
|-------|------|----------|-----------|
| **LSTM** | Deep Learning | Time-series forecasting | Captures long-term dependencies, sequential patterns |
| **XGBoost** | Gradient Boosting | Tabular prediction | Feature importance, handles missing data, fast training |
| **ARIMA** | Statistical | Baseline forecasting | Interpretable, established methodology |
| **Random Forest** | Ensemble | Alternative comparison | Robust, feature importance |

### Evaluation Metrics

| Metric | Formula | Target | Interpretation |
|--------|---------|--------|----------------|
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | < 5.0 | Average absolute error |
| **RMSE** | $\sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$ | < 8.0 | Root mean squared error |
| **MAPE** | $\frac{100}{n}\sum\|\frac{y_i - \hat{y}_i}{y_i}\|$ | < 15% | Mean absolute percentage error |
| **RÂ²** | $1 - \frac{SS_{res}}{SS_{tot}}$ | > 0.75 | Coefficient of determination |

---

## ğŸ¯ Success Criteria

### By End of Week 5

- [ ] Feature engineering completed
- [ ] At least 2 models trained
- [ ] Initial evaluation metrics collected
- [ ] Model comparison started

### By End of Week 6

- [ ] All 3+ models trained and validated
- [ ] Hyperparameter tuning completed
- [ ] Cross-validation results documented
- [ ] Feature importance analysis complete
- [ ] Key insights documented
- [ ] All notebooks reproducible
- [ ] Ready for T4 (Presentation)

---

## ğŸ”— Important Links

### Daily Use
- ğŸ“… [Week 3 Checklist](Week_3_Modeling_Analysis_Evaluation_Checklist_Template.md) - Task tracking
- ğŸ“Š [Model Config](08_Configuration/model_config.yaml) - Configurations
- ğŸ“ˆ [Evaluation Metrics](03_Model_Evaluation/Evaluation_Metrics.md) - Performance

### T2 Reference
- ğŸ“– [T2 Data Dictionary](../T2/05_Documentation/03_Technical_Docs/02_Data_Dictionary.md) - Data definitions
- ğŸ“Š [T2 EDA Findings](../T2/05_Documentation/03_Technical_Docs/03_EDA_Findings.md) - EDA results
- ğŸ“ [T2 Cleaned Data](../T2/02_Data/Processed/) - Input data

### Code & Setup
- ğŸ [Scripts](05_Scripts/) - Python modules
- ğŸ““ [Notebooks](06_Notebooks/) - Jupyter notebooks
- âš™ï¸ [Configuration](08_Configuration/) - Setup files

---

## ğŸ“ Team Contacts

| Role | Name | Student ID | Responsibility |
|------|------|-----------|-----------------|
| **Project Manager** | à¸™à¸´à¸•à¸´à¸ à¸¹à¸¡à¸´ à¹‚à¸à¸˜à¸´à¸Šà¸±à¸¢ | 66109010194 | Coordination, tracking |
| **Data Analyst** | à¸§à¸µà¸£à¹Œà¸à¸§à¸´à¸™ à¸™à¸²à¸„à¸™à¸´à¸˜à¸´à¸Šà¸±à¸¢à¸£à¸±à¸Šà¸•à¹Œ | 66109010201 | Features, visualization |
| **Data Scientist** | à¸„à¸²à¸¡à¸´à¸™ à¸ªà¸¸à¸£à¸‚à¸ˆà¸£ | 66109010322 | Modeling, evaluation |
| **Visualization** | à¸¢à¸¨à¸§à¸µà¸£à¹Œ à¸à¸´à¸¡à¸à¹Œà¸£à¸±à¸à¹€à¸à¸©à¸¡ | 66109010455 | Results, documentation |
| **Technical Lead** | à¸à¸¤à¸•à¸ à¸²à¸ª à¸­à¸´à¹ˆà¸¡à¸—à¸±à¹ˆà¸§ | 66109010180 | QA, reproducibility |

---

## ğŸ“ˆ Progress Tracking

### Phase 3 Timeline

```
Week 5: Feature Engineering & Initial Modeling
â”œâ”€â”€ Mon-Tue: Feature engineering completion
â”œâ”€â”€ Wed-Thu: LSTM and XGBoost training
â””â”€â”€ Fri: Initial evaluation & review

Week 6: Advanced Modeling & Evaluation
â”œâ”€â”€ Mon-Wed: Hyperparameter tuning
â”œâ”€â”€ Thu: Cross-validation & comparison
â””â”€â”€ Fri: Documentation & insights
```

---

## ğŸ“ Best Practices

### Model Development
1. **Always set random seeds** for reproducibility
2. **Use temporal splits** for time-series (no data leakage)
3. **Document all hyperparameters** and their rationale
4. **Save model checkpoints** during training
5. **Version control** model artifacts

### Evaluation
1. **Use multiple metrics** (not just accuracy)
2. **Cross-validate** with proper folds
3. **Test on truly unseen data** (holdout set)
4. **Compare against baselines** (naive, mean)
5. **Document limitations** honestly

### Code Quality
1. **Follow PEP 8** style guidelines
2. **Write docstrings** for all functions
3. **Log important events** (training, evaluation)
4. **Handle errors gracefully** with try-except
5. **Create reusable functions** in scripts

---

## ğŸ†˜ Getting Help

### If You Have Questions

**About modeling:**
â†’ Check [Model_Selection_Justification.md](01_Model_Selection/Model_Selection_Justification.md)  
â†’ Ask Data Scientist (à¸„à¸²à¸¡à¸´à¸™)

**About features:**
â†’ Check [Feature_Importance.md](04_Analysis/Feature_Importance.md)  
â†’ Ask Data Analyst (à¸§à¸µà¸£à¹Œà¸à¸§à¸´à¸™)

**About code:**
â†’ Check [05_Scripts/README.md](05_Scripts/README.md)  
â†’ Ask Technical Lead (à¸à¸¤à¸•à¸ à¸²à¸ª)

**About tasks:**
â†’ Check [Week_3_Checklist](Week_3_Modeling_Analysis_Evaluation_Checklist_Template.md)  
â†’ Ask Project Manager (à¸™à¸´à¸•à¸´à¸ à¸¹à¸¡à¸´)

---

**Last Updated:** November 27, 2025

**Status:** Ready for Phase 3 Work

**Previous Phase:** [T2 - Data Collection, Cleaning & EDA](../T2/README.md)

**Next Phase:** T4 - Presentation & Visualization

---

**You've got this! Good luck with T3! ğŸš€**
