# Best Hyperparameters
# Bangkok Traffic Flow Optimization Project
# Updated: November 27, 2025

# LSTM Model
lstm:
  architecture:
    units_layer1: 64
    units_layer2: 32
    dropout: 0.2
    activation: 'relu'
  training:
    epochs: 100
    batch_size: 32
    optimizer: 'adam'
    learning_rate: 0.001
  data:
    sequence_length: 7
    validation_split: 0.2
  callbacks:
    early_stopping:
      patience: 10
      restore_best_weights: true
    reduce_lr:
      factor: 0.5
      patience: 5

# XGBoost Model
xgboost:
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  min_child_weight: 1
  gamma: 0
  reg_alpha: 0
  reg_lambda: 1
  objective: 'reg:squarederror'
  eval_metric: 'rmse'
  random_state: 42

# ARIMA Model
arima:
  order:
    p: 1
    d: 1
    q: 1
  seasonal_order:
    P: 1
    D: 1
    Q: 1
    s: 7
  enforce_stationarity: true
  enforce_invertibility: true

# Random Forest Model
random_forest:
  n_estimators: 100
  max_depth: 15
  min_samples_split: 5
  min_samples_leaf: 2
  max_features: 'sqrt'
  bootstrap: true
  random_state: 42
  n_jobs: -1

# Status: To be updated after tuning
tuning_status: 'pending'
last_tuning_date: null
