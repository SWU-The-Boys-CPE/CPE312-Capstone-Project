{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad13536",
   "metadata": {},
   "source": [
    "# üìà Model Evaluation for Traffic Flow Optimization\n",
    "\n",
    "**Phase 3: Modeling, Analysis, and Evaluation**\n",
    "\n",
    "## Overview\n",
    "This notebook evaluates trained models using comprehensive metrics:\n",
    "- MAE, RMSE, MAPE, R¬≤\n",
    "- Cross-validation performance\n",
    "- Model comparison\n",
    "- Visualization of predictions vs actuals\n",
    "\n",
    "**Target Metrics:**\n",
    "- MAE < 5.0\n",
    "- RMSE < 8.0\n",
    "- MAPE < 15%\n",
    "- R¬≤ > 0.75\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Data Science Team  \n",
    "**Date:** November 2025  \n",
    "**Project:** Bangkok Traffic Flow Optimization (CPE312 Capstone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d7fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add T3 scripts to path\n",
    "import sys\n",
    "sys.path.append('../05_Scripts')\n",
    "\n",
    "# Import custom modules\n",
    "from evaluation import (\n",
    "    calculate_all_metrics,\n",
    "    compare_models,\n",
    "    generate_evaluation_report,\n",
    "    calculate_improvement_over_baseline\n",
    ")\n",
    "from model_utils import load_model\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452479f",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d91c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 331\n",
      "Features: 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/extra_trees_tuned.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/scaler.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/random_forest_model.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/xgboost_tuned.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/gradient_boosting_tuned.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: extra_trees_tuned\n",
      "‚úÖ Loaded: scaler\n",
      "‚úÖ Loaded: random_forest\n",
      "‚úÖ Loaded: xgboost_tuned\n",
      "‚úÖ Loaded: gradient_boosting_tuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/arima_model.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/random_forest_tuned.pkl\n",
      "INFO:model_utils:Model loaded from ../02_Model_Development/Trained_Models/xgboost_model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: arima\n",
      "‚úÖ Loaded: random_forest_tuned\n",
      "‚úÖ Loaded: xgboost\n",
      "\n",
      "Total models loaded: 8\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path('../02_Data/Processed/')\n",
    "MODEL_PATH = Path('../02_Model_Development/Trained_Models/')\n",
    "RESULTS_PATH = Path('../09_Results/')\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_PATH / 'Figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv(DATA_PATH / 'features_engineered.csv')\n",
    "target_col = 'congestion_index'\n",
    "\n",
    "# Select only numeric columns for features (exclude date and target)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "\n",
    "# Handle NaN values\n",
    "df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "# Split to get test set (last 20%)\n",
    "n = len(df)\n",
    "test_start = int(n * 0.8)\n",
    "test_df = df.iloc[test_start:]\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "# Load trained models\n",
    "models = {}\n",
    "for model_file in MODEL_PATH.glob('*.pkl'):\n",
    "    model_name = model_file.stem.replace('_model', '')\n",
    "    models[model_name] = load_model(str(model_file))\n",
    "    print(f\"‚úÖ Loaded: {model_name}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d745d3",
   "metadata": {},
   "source": [
    "## 2. Evaluate Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1209f6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: extra_trees_tuned\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 33 features, but ExtraTreesRegressor is expecting 35 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mforecast\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     12\u001b[39m     y_pred = model.forecast(\u001b[38;5;28mlen\u001b[39m(y_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/T9/Documents/01-computer-engineer/Y3-TR1/CPE312/Capstone Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:1065\u001b[39m, in \u001b[36mForestRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1063\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[32m   1068\u001b[39m n_jobs, _, _ = _partition_estimators(\u001b[38;5;28mself\u001b[39m.n_estimators, \u001b[38;5;28mself\u001b[39m.n_jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/T9/Documents/01-computer-engineer/Y3-TR1/CPE312/Capstone Project/.venv/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:637\u001b[39m, in \u001b[36mBaseForest._validate_X_predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    635\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X.indices.dtype != np.intc \u001b[38;5;129;01mor\u001b[39;00m X.indptr.dtype != np.intc):\n\u001b[32m    646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/T9/Documents/01-computer-engineer/Y3-TR1/CPE312/Capstone Project/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/T9/Documents/01-computer-engineer/Y3-TR1/CPE312/Capstone Project/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 33 features, but ExtraTreesRegressor is expecting 35 features as input."
     ]
    }
   ],
   "source": [
    "# Evaluate each model\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    if hasattr(model, 'predict'):\n",
    "        y_pred = model.predict(X_test)\n",
    "    elif hasattr(model, 'forecast'):\n",
    "        y_pred = model.forecast(len(y_test))\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Model has no predict method\")\n",
    "        continue\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_all_metrics(y_test, y_pred)\n",
    "    results[name] = metrics\n",
    "    \n",
    "    print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "    print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
    "    print(f\"  R¬≤:   {metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45699e",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c03239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON (sorted by RMSE)\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df)\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv(RESULTS_PATH / 'model_comparison.csv')\n",
    "print(f\"\\n‚úÖ Comparison saved to: {RESULTS_PATH / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b78a1",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d112644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. RMSE comparison bar chart\n",
    "ax1 = axes[0, 0]\n",
    "comparison_df['RMSE'].plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.axhline(y=8.0, color='red', linestyle='--', label='Target (8.0)')\n",
    "ax1.set_title('RMSE by Model')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. R¬≤ comparison bar chart\n",
    "ax2 = axes[0, 1]\n",
    "comparison_df['R2'].plot(kind='bar', ax=ax2, color='forestgreen')\n",
    "ax2.axhline(y=0.75, color='red', linestyle='--', label='Target (0.75)')\n",
    "ax2.set_title('R¬≤ by Model')\n",
    "ax2.set_ylabel('R¬≤')\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Predictions vs Actuals (best model)\n",
    "ax3 = axes[1, 0]\n",
    "best_model = comparison_df['RMSE'].idxmin()\n",
    "if best_model in predictions:\n",
    "    ax3.plot(y_test[:100], label='Actual', color='blue', alpha=0.7)\n",
    "    ax3.plot(predictions[best_model][:100], label=f'{best_model} Prediction', color='orange', alpha=0.7)\n",
    "    ax3.set_title(f'Predictions vs Actuals ({best_model})')\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_ylabel('Value')\n",
    "    ax3.legend()\n",
    "\n",
    "# 4. Error distribution\n",
    "ax4 = axes[1, 1]\n",
    "if best_model in predictions:\n",
    "    errors = y_test - predictions[best_model]\n",
    "    ax4.hist(errors, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--')\n",
    "    ax4.set_title(f'Error Distribution ({best_model})')\n",
    "    ax4.set_xlabel('Prediction Error')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'Figures' / 'model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to: {RESULTS_PATH / 'Figures' / 'model_evaluation.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02058ded",
   "metadata": {},
   "source": [
    "## 5. Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_model = comparison_df['RMSE'].idxmin()\n",
    "best_metrics = results[best_model]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  MAE:  {best_metrics['MAE']:.4f} {'‚úÖ' if best_metrics['MAE'] < 5.0 else '‚ö†Ô∏è'}\")\n",
    "print(f\"  RMSE: {best_metrics['RMSE']:.4f} {'‚úÖ' if best_metrics['RMSE'] < 8.0 else '‚ö†Ô∏è'}\")\n",
    "print(f\"  MAPE: {best_metrics['MAPE']:.2f}% {'‚úÖ' if best_metrics['MAPE'] < 15 else '‚ö†Ô∏è'}\")\n",
    "print(f\"  R¬≤:   {best_metrics['R2']:.4f} {'‚úÖ' if best_metrics['R2'] > 0.75 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Next Step: Run 07_Model_Interpretation.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
