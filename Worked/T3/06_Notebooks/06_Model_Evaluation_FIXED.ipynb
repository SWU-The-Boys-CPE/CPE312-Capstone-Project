{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad13536",
   "metadata": {},
   "source": [
    "# üìà Model Evaluation for Traffic Flow Optimization\n",
    "\n",
    "**Phase 3: Modeling, Analysis, and Evaluation**\n",
    "\n",
    "## Overview\n",
    "This notebook evaluates trained models using comprehensive metrics:\n",
    "- MAE, RMSE, MAPE, R¬≤\n",
    "- Cross-validation performance\n",
    "- Model comparison\n",
    "- Visualization of predictions vs actuals\n",
    "\n",
    "**Target Metrics:**\n",
    "- MAE < 5.0\n",
    "- RMSE < 8.0\n",
    "- MAPE < 15%\n",
    "- R¬≤ > 0.75\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Data Science Team  \n",
    "**Date:** November 2025  \n",
    "**Project:** Bangkok Traffic Flow Optimization (CPE312 Capstone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add T3 scripts to path\n",
    "import sys\n",
    "sys.path.append('../05_Scripts')\n",
    "\n",
    "# Import custom modules\n",
    "from evaluation import (\n",
    "    calculate_all_metrics,\n",
    "    compare_models,\n",
    "    generate_evaluation_report,\n",
    "    calculate_improvement_over_baseline\n",
    ")\n",
    "from model_utils import load_model\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452479f",
   "metadata": {},
   "source": [
    "## 1. Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_PATH = Path('../02_Data/Processed/')\n",
    "MODEL_PATH = Path('../02_Model_Development/Trained_Models/')\n",
    "RESULTS_PATH = Path('../09_Results/')\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_PATH / 'Figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load test data\n",
    "df = pd.read_csv(DATA_PATH / 'features_engineered.csv')\n",
    "target_col = 'congestion_index'\n",
    "\n",
    "# Get ALL numeric columns except date and target\n",
    "all_cols = df.columns.tolist()\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Exclude target from features\n",
    "feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "\n",
    "# Handle NaN values\n",
    "df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "# Split to get test set (last 20%)\n",
    "n = len(df)\n",
    "test_start = int(n * 0.8)\n",
    "test_df = df.iloc[test_start:]\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# Load trained models - SKIP models that expect different number of features\n",
    "models = {}\n",
    "for model_file in MODEL_PATH.glob('*.pkl'):\n",
    "    model_name = model_file.stem.replace('_model', '')\n",
    "    try:\n",
    "        model = load_model(str(model_file))\n",
    "        # Check if model has n_features_in_ attribute\n",
    "        if hasattr(model, 'n_features_in_'):\n",
    "            if model.n_features_in_ != len(feature_cols):\n",
    "                print(f\"  ‚ö†Ô∏è Skipping {model_name}: expects {model.n_features_in_} features but data has {len(feature_cols)}\")\n",
    "                continue\n",
    "        models[model_name] = model\n",
    "        print(f\"‚úÖ Loaded: {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading {model_name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d745d3",
   "metadata": {},
   "source": [
    "## 2. Evaluate Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    if hasattr(model, 'predict'):\n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Prediction error: {str(e)}\")\n",
    "            continue\n",
    "    elif hasattr(model, 'forecast'):\n",
    "        try:\n",
    "            y_pred = model.forecast(len(y_test))\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Forecast error: {str(e)}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Model has no predict method\")\n",
    "        continue\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        metrics = calculate_all_metrics(y_test, y_pred)\n",
    "        results[name] = metrics\n",
    "        \n",
    "        print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
    "        print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
    "        print(f\"  MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        print(f\"  R¬≤:   {metrics['R2']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Metrics calculation error: {str(e)}\")\n",
    "\n",
    "if not results:\n",
    "    print(\"\\n‚ö†Ô∏è No models were successfully evaluated!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Successfully evaluated {len(results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45699e",
   "metadata": {},
   "source": [
    "## 3. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c03239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "if results:\n",
    "    comparison_df = pd.DataFrame(results).T\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL COMPARISON (sorted by RMSE)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Save to CSV\n",
    "    comparison_df.to_csv(RESULTS_PATH / 'model_comparison.csv')\n",
    "    print(f\"\\n‚úÖ Comparison saved to: {RESULTS_PATH / 'model_comparison.csv'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to compare!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b78a1",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d112644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if results and comparison_df is not None and not comparison_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1. RMSE comparison bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    comparison_df['RMSE'].plot(kind='bar', ax=ax1, color='steelblue')\n",
    "    ax1.axhline(y=8.0, color='red', linestyle='--', label='Target (8.0)')\n",
    "    ax1.set_title('RMSE by Model')\n",
    "    ax1.set_ylabel('RMSE')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 2. R¬≤ comparison bar chart\n",
    "    ax2 = axes[0, 1]\n",
    "    comparison_df['R2'].plot(kind='bar', ax=ax2, color='forestgreen')\n",
    "    ax2.axhline(y=0.75, color='red', linestyle='--', label='Target (0.75)')\n",
    "    ax2.set_title('R¬≤ by Model')\n",
    "    ax2.set_ylabel('R¬≤')\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 3. Predictions vs Actuals (best model)\n",
    "    ax3 = axes[1, 0]\n",
    "    best_model = comparison_df['RMSE'].idxmin()\n",
    "    if best_model in predictions:\n",
    "        ax3.plot(y_test[:100], label='Actual', color='blue', alpha=0.7)\n",
    "        ax3.plot(predictions[best_model][:100], label=f'{best_model} Prediction', color='orange', alpha=0.7)\n",
    "        ax3.set_title(f'Predictions vs Actuals ({best_model})')\n",
    "        ax3.set_xlabel('Time')\n",
    "        ax3.set_ylabel('Value')\n",
    "        ax3.legend()\n",
    "\n",
    "    # 4. Error distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    if best_model in predictions:\n",
    "        errors = y_test - predictions[best_model]\n",
    "        ax4.hist(errors, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "        ax4.axvline(x=0, color='red', linestyle='--')\n",
    "        ax4.set_title(f'Error Distribution ({best_model})')\n",
    "        ax4.set_xlabel('Prediction Error')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'Figures' / 'model_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n‚úÖ Visualization saved to: {RESULTS_PATH / 'Figures' / 'model_evaluation.png'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping visualization - no results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02058ded",
   "metadata": {},
   "source": [
    "## 5. Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "if results:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    best_model = comparison_df['RMSE'].idxmin()\n",
    "    best_metrics = results[best_model]\n",
    "\n",
    "    print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  MAE:  {best_metrics['MAE']:.4f} {'‚úÖ' if best_metrics['MAE'] < 5.0 else '‚ö†Ô∏è'}\")\n",
    "    print(f\"  RMSE: {best_metrics['RMSE']:.4f} {'‚úÖ' if best_metrics['RMSE'] < 8.0 else '‚ö†Ô∏è'}\")\n",
    "    print(f\"  MAPE: {best_metrics['MAPE']:.2f}% {'‚úÖ' if best_metrics['MAPE'] < 15 else '‚ö†Ô∏è'}\")\n",
    "    print(f\"  R¬≤:   {best_metrics['R2']:.4f} {'‚úÖ' if best_metrics['R2'] > 0.75 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Next Step: Run 07_Model_Interpretation.ipynb\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL EVALUATION FAILED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"No models could be evaluated due to feature mismatch errors.\")\n",
    "    print(\"Please check that the data has the same features as when the models were trained.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
