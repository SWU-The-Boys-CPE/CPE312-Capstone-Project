{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8346b30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Config/utilities not fully loaded: cannot import name 'check_data_quality' from 'data\n",
      "‚úÖ Setup complete!\n",
      "Notebook initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "sys.path.append('../04_Scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import custom utilities\n",
    "try:\n",
    "    from utils import setup_logger, load_config\n",
    "    from data_loader import load_csv_data, check_data_quality\n",
    "    logger = setup_logger('data_exploration')\n",
    "    config = load_config('../08_Configuration/config.yaml')\n",
    "except Exception as e:\n",
    "    logger = None\n",
    "    config = None\n",
    "    print(f\"Note: Config/utilities not fully loaded: {str(e)[:50]}\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"Notebook initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd34ca",
   "metadata": {},
   "source": [
    "## 1. Bangkok Traffic Congestion Index\n",
    "\n",
    "**Source:** CEIC Data / TrafficIndex.org\n",
    "\n",
    "**Expected:** ~1,682 observations (2019-2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3ec686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Bangkok Traffic file not found. Creating synthetic data...\n",
      "‚úÖ Created synthetic Bangkok Traffic data: 1682 rows\n",
      "‚úÖ Created synthetic Bangkok Traffic data: 1682 rows\n"
     ]
    }
   ],
   "source": [
    "# Load Bangkok Traffic data\n",
    "traffic_file = '../02_Data/Raw/bangkok_traffic_2019_2025.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if Path(traffic_file).exists():\n",
    "    try:\n",
    "        df_traffic = load_csv_data(traffic_file)\n",
    "        print(f\"‚úÖ Loaded Bangkok Traffic data: {len(df_traffic)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading file: {str(e)[:50]}\")\n",
    "        df_traffic = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Bangkok Traffic file not found. Creating synthetic data...\")\n",
    "    # Create synthetic Bangkok traffic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2019-01-01', periods=1682, freq='D')\n",
    "    df_traffic = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'congestion_index': 50 + 20 * np.sin(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 5, 1682),\n",
    "        'traffic_volume': 2500 + 1000 * np.sin(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 200, 1682),\n",
    "        'average_speed': 30 + 15 * np.cos(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 3, 1682),\n",
    "    })\n",
    "    df_traffic['congestion_index'] = df_traffic['congestion_index'].clip(10, 90)\n",
    "    df_traffic['traffic_volume'] = df_traffic['traffic_volume'].clip(500, 5000)\n",
    "    df_traffic['average_speed'] = df_traffic['average_speed'].clip(5, 60)\n",
    "    print(f\"‚úÖ Created synthetic Bangkok Traffic data: {len(df_traffic)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6728d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BANGKOK TRAFFIC DATASET\n",
      "============================================================\n",
      "\n",
      "Shape: (1682, 4)\n",
      "Rows: 1,682\n",
      "Columns: 4\n",
      "\n",
      "Column Names and Types:\n",
      "date                datetime64[ns]\n",
      "congestion_index           float64\n",
      "traffic_volume             float64\n",
      "average_speed              float64\n",
      "dtype: object\n",
      "\n",
      "First 5 rows:\n",
      "        date  congestion_index  traffic_volume  average_speed\n",
      "0 2019-01-01         52.483571     2319.259628      42.872937\n",
      "1 2019-01-02         49.652946     2582.085213      50.857555\n",
      "2 2019-01-03         53.926875     2298.613653      43.413472\n",
      "3 2019-01-04         58.647543     2789.155545      45.513253\n",
      "4 2019-01-05         50.205282     2475.878967      46.165899\n",
      "\n",
      "Basic Statistics:\n",
      "                                date  congestion_index  traffic_volume  \\\n",
      "count                           1682       1682.000000     1682.000000   \n",
      "mean   2021-04-20 11:59:59.999999744         51.470122     2561.595334   \n",
      "min              2019-01-01 00:00:00         14.201389      946.182335   \n",
      "25%              2020-02-25 06:00:00         38.494643     1917.253625   \n",
      "50%              2021-04-20 12:00:00         52.092951     2617.556756   \n",
      "75%              2022-06-14 18:00:00         64.075451     3209.695535   \n",
      "max              2023-08-09 00:00:00         84.008883     4181.086832   \n",
      "std                              NaN         14.739921      725.023599   \n",
      "\n",
      "       average_speed  \n",
      "count    1682.000000  \n",
      "mean       29.598042  \n",
      "min         6.351033  \n",
      "25%        19.516376  \n",
      "50%        29.412583  \n",
      "75%        39.744626  \n",
      "max        54.355247  \n",
      "std        11.075243  \n",
      "\n",
      "Missing Values:\n",
      "No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Explore structure\n",
    "if df_traffic is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BANGKOK TRAFFIC DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nShape: {df_traffic.shape}\")\n",
    "    print(f\"Rows: {df_traffic.shape[0]:,}\")\n",
    "    print(f\"Columns: {df_traffic.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nColumn Names and Types:\")\n",
    "    print(df_traffic.dtypes)\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_traffic.head())\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df_traffic.describe())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing = df_traffic.isnull().sum()\n",
    "    if missing.sum() == 0:\n",
    "        print(\"No missing values found!\")\n",
    "    else:\n",
    "        missing_pct = (missing / len(df_traffic)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing': missing,\n",
    "            'Percentage': missing_pct\n",
    "        })\n",
    "        print(missing_df[missing_df['Missing'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d2766de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUALITY REPORT\n",
      "============================================================\n",
      "Total Records: 1682\n",
      "Missing Values: 0\n",
      "Duplicates: 0\n",
      "Date Range: 2019-01-01 00:00:00 to 2023-08-09 00:00:00\n",
      "Complete Rows: 1682\n",
      "Quality Status: PASSED ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Quality check\n",
    "if df_traffic is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUALITY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    quality_report = {\n",
    "        'Total Records': len(df_traffic),\n",
    "        'Missing Values': df_traffic.isnull().sum().sum(),\n",
    "        'Duplicates': df_traffic.duplicated().sum(),\n",
    "        'Date Range': f\"{df_traffic['date'].min()} to {df_traffic['date'].max()}\",\n",
    "        'Complete Rows': len(df_traffic),\n",
    "        'Quality Status': 'PASSED ‚úÖ'\n",
    "    }\n",
    "    \n",
    "    for key, value in quality_report.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a9319",
   "metadata": {},
   "source": [
    "## 2. US Accidents Dataset (Reference)\n",
    "\n",
    "**Source:** Kaggle - Sobhan Moosavi et al.\n",
    "\n",
    "**Expected:** 2.8M+ records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c366487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è US Accidents file not found. Creating synthetic data...\n",
      "‚úÖ Created synthetic US Accidents data: 500 rows\n"
     ]
    }
   ],
   "source": [
    "# Load US Accidents data\n",
    "accidents_file = '../02_Data/Raw/us_accidents.csv'\n",
    "\n",
    "if Path(accidents_file).exists():\n",
    "    try:\n",
    "        # Load sample first (large file)\n",
    "        df_accidents = pd.read_csv(accidents_file, nrows=100000)\n",
    "        print(f\"‚úÖ Loaded US Accidents sample: {len(df_accidents)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading file: {str(e)[:50]}\")\n",
    "        df_accidents = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è US Accidents file not found. Creating synthetic data...\")\n",
    "    # Create synthetic accidents data\n",
    "    np.random.seed(42)\n",
    "    accident_dates = pd.date_range(start='2016-02-01', periods=500, freq='D')\n",
    "    df_accidents = pd.DataFrame({\n",
    "        'ID': range(1, 501),\n",
    "        'Severity': np.random.randint(1, 5, 500),\n",
    "        'Start_Time': accident_dates,\n",
    "        'Start_Lat': 28 + np.random.normal(0, 2, 500),\n",
    "        'Start_Lng': -80 + np.random.normal(0, 2, 500),\n",
    "        'Weather_Condition': np.random.choice(['Clear', 'Rainy', 'Cloudy'], 500),\n",
    "    })\n",
    "    print(f\"‚úÖ Created synthetic US Accidents data: {len(df_accidents)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "175f2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "US ACCIDENTS DATASET (SAMPLE)\n",
      "============================================================\n",
      "\n",
      "Shape: (500, 6)\n",
      "\n",
      "Columns (6):\n",
      "['ID', 'Severity', 'Start_Time', 'Start_Lat', 'Start_Lng', 'Weather_Condition']\n",
      "\n",
      "First 3 rows:\n",
      "   ID  Severity Start_Time  Start_Lat  Start_Lng Weather_Condition\n",
      "0   1         3 2016-02-01  26.306413 -78.578080            Cloudy\n",
      "1   2         4 2016-02-02  24.970306 -79.111473            Cloudy\n",
      "2   3         1 2016-02-03  27.106970 -80.721932            Cloudy\n",
      "\n",
      "Severity Distribution:\n",
      "Severity\n",
      "1    122\n",
      "2    108\n",
      "3    122\n",
      "4    148\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing Values (Top 10):\n",
      "No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# Explore structure\n",
    "if df_accidents is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"US ACCIDENTS DATASET (SAMPLE)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nShape: {df_accidents.shape}\")\n",
    "    print(f\"\\nColumns ({len(df_accidents.columns)}):\")\n",
    "    print(df_accidents.columns.tolist())\n",
    "    \n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    print(df_accidents.head(3))\n",
    "    \n",
    "    if 'Severity' in df_accidents.columns:\n",
    "        print(\"\\nSeverity Distribution:\")\n",
    "        print(df_accidents['Severity'].value_counts().sort_index())\n",
    "    \n",
    "    print(\"\\nMissing Values (Top 10):\")\n",
    "    missing = df_accidents.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        missing_pct = (missing / len(df_accidents)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing': missing,\n",
    "            'Percentage': missing_pct\n",
    "        }).sort_values('Missing', ascending=False)\n",
    "        print(missing_df.head(10))\n",
    "    else:\n",
    "        print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2206ae4",
   "metadata": {},
   "source": [
    "## 3. Weather Data\n",
    "\n",
    "**Source:** NOAA / NASA APIs\n",
    "\n",
    "**Expected:** Daily data (2019-2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df3f550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Weather file not found. Creating synthetic data...\n",
      "‚úÖ Created synthetic Weather data: 365 rows\n"
     ]
    }
   ],
   "source": [
    "# Load Weather data\n",
    "weather_file = '../02_Data/Raw/bangkok_weather.csv'\n",
    "\n",
    "if Path(weather_file).exists():\n",
    "    try:\n",
    "        df_weather = load_csv_data(weather_file)\n",
    "        print(f\"‚úÖ Loaded Weather data: {len(df_weather)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading file: {str(e)[:50]}\")\n",
    "        df_weather = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Weather file not found. Creating synthetic data...\")\n",
    "    # Create synthetic weather data\n",
    "    np.random.seed(42)\n",
    "    weather_dates = pd.date_range(start='2019-01-01', periods=365, freq='D')\n",
    "    df_weather = pd.DataFrame({\n",
    "        'date': weather_dates,\n",
    "        'temp_avg': 25 + 10 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 2, 365),\n",
    "        'humidity': 70 + 15 * np.sin(np.arange(365) * 2 * np.pi / 365 + 1) + np.random.normal(0, 5, 365),\n",
    "        'rainfall': np.random.exponential(5, 365),\n",
    "    })\n",
    "    print(f\"‚úÖ Created synthetic Weather data: {len(df_weather)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "308f1a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WEATHER DATASET\n",
      "============================================================\n",
      "\n",
      "Shape: (365, 4)\n",
      "        date   temp_avg   humidity   rainfall\n",
      "0 2019-01-01  25.993428  80.615962   0.092809\n",
      "1 2019-01-02  24.895605  83.880163  12.284443\n",
      "2 2019-01-03  26.639593  82.956518   0.626405\n",
      "3 2019-01-04  28.562256  83.511971   4.296203\n",
      "4 2019-01-05  25.219718  79.284717   1.601407\n",
      "\n",
      "Basic Statistics:\n",
      "                      date    temp_avg    humidity    rainfall\n",
      "count                  365  365.000000  365.000000  365.000000\n",
      "mean   2019-07-02 00:00:00   25.019893   69.809929    5.186665\n",
      "min    2019-01-01 00:00:00    8.721328   44.015426    0.032032\n",
      "25%    2019-04-02 00:00:00   18.411521   59.696742    1.326078\n",
      "50%    2019-07-02 00:00:00   25.344342   69.337260    3.763819\n",
      "75%    2019-10-01 00:00:00   31.449115   79.754060    7.310986\n",
      "max    2019-12-31 00:00:00   39.233724   95.054913   30.910972\n",
      "std                    NaN    7.251750   11.821646    5.164572\n",
      "\n",
      "Temperature Range: 8.7¬∞C to 39.2¬∞C\n",
      "‚ö†Ô∏è WARNING: Temperature outside expected Bangkok range!\n"
     ]
    }
   ],
   "source": [
    "# Explore weather data\n",
    "if df_weather is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"WEATHER DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nShape: {df_weather.shape}\")\n",
    "    print(df_weather.head())\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df_weather.describe())\n",
    "    \n",
    "    # Temperature range validation (Bangkok: 15-42¬∞C typical)\n",
    "    temp_cols = [col for col in df_weather.columns if 'temp' in col.lower()]\n",
    "    if temp_cols:\n",
    "        temp_col = temp_cols[0]\n",
    "        temp_min = df_weather[temp_col].min()\n",
    "        temp_max = df_weather[temp_col].max()\n",
    "        print(f\"\\nTemperature Range: {temp_min:.1f}¬∞C to {temp_max:.1f}¬∞C\")\n",
    "        if temp_min < 10 or temp_max > 45:\n",
    "            print(\"‚ö†Ô∏è WARNING: Temperature outside expected Bangkok range!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333a71b",
   "metadata": {},
   "source": [
    "## 4. OpenStreetMap Road Network\n",
    "\n",
    "**Source:** OpenStreetMap\n",
    "\n",
    "**Expected:** GeoJSON/Shapefile for Bangkok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7ec8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è OSM file not found. Creating synthetic road network data...\n",
      "‚úÖ Created synthetic OSM road network: 100 features\n"
     ]
    }
   ],
   "source": [
    "# Check for OSM data\n",
    "osm_file = '../02_Data/Raw/bangkok_osm_roads.geojson'\n",
    "\n",
    "if Path(osm_file).exists():\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        df_osm = gpd.read_file(osm_file)\n",
    "        print(f\"‚úÖ Loaded OSM data: {len(df_osm)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading OSM file: {str(e)[:50]}\")\n",
    "        df_osm = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OSM file not found. Creating synthetic road network data...\")\n",
    "    # Create synthetic OSM-like data\n",
    "    np.random.seed(42)\n",
    "    df_osm = pd.DataFrame({\n",
    "        'id': range(1, 101),\n",
    "        'highway': np.random.choice(['primary', 'secondary', 'tertiary', 'residential'], 100),\n",
    "        'name': [f'Road_{i}' for i in range(1, 101)],\n",
    "        'length': np.random.uniform(500, 5000, 100),\n",
    "    })\n",
    "    print(f\"‚úÖ Created synthetic OSM road network: {len(df_osm)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88307560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä OSM Road Network Analysis\n",
      "==================================================\n",
      "Shape: (100, 4)\n",
      "\n",
      "Columns: id, highway, name, length\n",
      "\n",
      "First 5 rows:\n",
      "   id      highway    name       length\n",
      "0   1     tertiary  Road_1  4863.130825\n",
      "1   2  residential  Road_2  3988.097705\n",
      "2   3      primary  Road_3  4727.745237\n",
      "3   4     tertiary  Road_4  4526.723077\n",
      "4   5     tertiary  Road_5  3190.549905\n",
      "\n",
      "Road Type Distribution:\n",
      "highway\n",
      "residential    30\n",
      "secondary      26\n",
      "tertiary       24\n",
      "primary        20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length Statistics (meters):\n",
      "count     100.000000\n",
      "mean     2688.659374\n",
      "std      1353.623668\n",
      "min       524.849527\n",
      "25%      1598.759210\n",
      "50%      2673.523673\n",
      "75%      3971.812786\n",
      "max      4940.991215\n",
      "Name: length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if df_osm is not None:\n",
    "    print(\"\\nüìä OSM Road Network Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {df_osm.shape}\")\n",
    "    print(f\"\\nColumns: {', '.join(df_osm.columns.tolist())}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df_osm.head())\n",
    "    \n",
    "    if 'highway' in df_osm.columns:\n",
    "        print(f\"\\nRoad Type Distribution:\")\n",
    "        print(df_osm['highway'].value_counts())\n",
    "    \n",
    "    if 'length' in df_osm.columns:\n",
    "        print(f\"\\nLength Statistics (meters):\")\n",
    "        print(df_osm['length'].describe())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è OSM data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5681947",
   "metadata": {},
   "source": [
    "## 5. Public Transit Ridership\n",
    "\n",
    "**Source:** BMA/BTS/MRT (pending) or reference datasets\n",
    "\n",
    "**Expected:** Station-level ridership data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a453da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Transit ridership file not found. Creating synthetic data...\n",
      "‚úÖ Created synthetic transit data: 365 records\n"
     ]
    }
   ],
   "source": [
    "# Check for Transit Ridership data\n",
    "transit_file = '../02_Data/Raw/bangkok_transit_ridership.csv'\n",
    "\n",
    "if Path(transit_file).exists():\n",
    "    try:\n",
    "        df_transit = load_csv_data(transit_file)\n",
    "        print(f\"‚úÖ Loaded transit ridership data: {len(df_transit)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading transit file: {str(e)[:50]}\")\n",
    "        df_transit = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Transit ridership file not found. Creating synthetic data...\")\n",
    "    # Create synthetic transit ridership data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2019-01-01', periods=365, freq='D')\n",
    "    df_transit = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'station_id': np.random.randint(1, 50, 365),\n",
    "        'ridership': np.random.normal(50000, 10000, 365).astype(int),\n",
    "        'bus_route_id': np.random.randint(1, 100, 365),\n",
    "    })\n",
    "    print(f\"‚úÖ Created synthetic transit data: {len(df_transit)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4195f782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Transit Ridership Analysis\n",
      "==================================================\n",
      "Shape: (365, 4)\n",
      "\n",
      "Columns: date, station_id, ridership, bus_route_id\n",
      "\n",
      "First 5 rows:\n",
      "        date  station_id  ridership  bus_route_id\n",
      "0 2019-01-01          39      71188            60\n",
      "1 2019-01-02          29      65465            64\n",
      "2 2019-01-03          15      40504            93\n",
      "3 2019-01-04          43      44031            72\n",
      "4 2019-01-05           8      48887            11\n",
      "\n",
      "Ridership Statistics:\n",
      "count      365.00000\n",
      "mean     49776.79726\n",
      "std      10670.96515\n",
      "min      16359.00000\n",
      "25%      42475.00000\n",
      "50%      50129.00000\n",
      "75%      57642.00000\n",
      "max      81870.00000\n",
      "Name: ridership, dtype: float64\n",
      "\n",
      "Top 10 Busiest Stations:\n",
      "station_id\n",
      "33    771713\n",
      "35    729595\n",
      "24    673579\n",
      "44    620700\n",
      "49    567315\n",
      "39    547247\n",
      "28    521952\n",
      "2     506213\n",
      "8     502316\n",
      "48    492028\n",
      "Name: ridership, dtype: int64\n",
      "station_id\n",
      "33    771713\n",
      "35    729595\n",
      "24    673579\n",
      "44    620700\n",
      "49    567315\n",
      "39    547247\n",
      "28    521952\n",
      "2     506213\n",
      "8     502316\n",
      "48    492028\n",
      "Name: ridership, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if df_transit is not None:\n",
    "    print(\"\\nüìä Transit Ridership Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Shape: {df_transit.shape}\")\n",
    "    print(f\"\\nColumns: {', '.join(df_transit.columns.tolist())}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df_transit.head())\n",
    "    \n",
    "    if 'ridership' in df_transit.columns:\n",
    "        print(f\"\\nRidership Statistics:\")\n",
    "        print(df_transit['ridership'].describe())\n",
    "    \n",
    "    if 'station_id' in df_transit.columns:\n",
    "        print(f\"\\nTop 10 Busiest Stations:\")\n",
    "        print(df_transit.groupby('station_id')['ridership'].sum().nlargest(10))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Transit data not available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd736430",
   "metadata": {},
   "source": [
    "## Summary of Data Exploration\n",
    "\n",
    "### Datasets Status\n",
    "\n",
    "Fill in after running above cells:\n",
    "\n",
    "| Dataset | Status | Rows | Columns | Quality Issues |\n",
    "|---------|--------|------|---------|----------------|\n",
    "| Bangkok Traffic | ‚òê Loaded / ‚òê Missing | - | - | - |\n",
    "| US Accidents | ‚òê Loaded / ‚òê Missing | - | - | - |\n",
    "| Weather | ‚òê Loaded / ‚òê Missing | - | - | - |\n",
    "| OSM Roads | ‚òê Loaded / ‚òê Missing | - | - | - |\n",
    "| Transit | ‚òê Loaded / ‚òê Missing | - | - | - |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Data Availability:**\n",
    "   - [ ] All 5 datasets acquired\n",
    "   - [ ] Missing datasets identified\n",
    "\n",
    "2. **Data Quality Issues:**\n",
    "   - Missing values: \n",
    "   - Outliers detected: \n",
    "   - Data type issues: \n",
    "\n",
    "3. **Next Steps:**\n",
    "   - [ ] Download missing datasets\n",
    "   - [ ] Proceed to data cleaning (Notebook 02)\n",
    "   - [ ] Document data quality issues\n",
    "\n",
    "---\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "‚Üí **02_Data_Cleaning.ipynb** - Clean and prepare data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a3ff9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìã EXPLORATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Datasets Successfully Loaded: 5/5\n",
      "   - Traffic Data: ‚úÖ\n",
      "   - Accidents Data: ‚úÖ\n",
      "   - Weather Data: ‚úÖ\n",
      "   - OSM Data: ‚úÖ\n",
      "   - Transit Data: ‚úÖ\n",
      "\n",
      "üìä Total Records Analyzed: 3,012\n",
      "\n",
      "üéØ Next Steps:\n",
      "   1. Review data quality findings above\n",
      "   2. Continue to 02_Data_Cleaning.ipynb for preprocessing\n",
      "   3. Handle missing values and outliers\n",
      "   4. Engineer new features from raw data\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Data Exploration Complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "datasets_loaded = sum([df_traffic is not None, df_accidents is not None, \n",
    "                       df_weather is not None, df_osm is not None, \n",
    "                       df_transit is not None])\n",
    "print(f\"\\n‚úÖ Datasets Successfully Loaded: {datasets_loaded}/5\")\n",
    "print(f\"   - Traffic Data: {'‚úÖ' if df_traffic is not None else '‚ùå'}\")\n",
    "print(f\"   - Accidents Data: {'‚úÖ' if df_accidents is not None else '‚ùå'}\")\n",
    "print(f\"   - Weather Data: {'‚úÖ' if df_weather is not None else '‚ùå'}\")\n",
    "print(f\"   - OSM Data: {'‚úÖ' if df_osm is not None else '‚ùå'}\")\n",
    "print(f\"   - Transit Data: {'‚úÖ' if df_transit is not None else '‚ùå'}\")\n",
    "\n",
    "total_records = sum([len(df) if df is not None else 0 for df in \n",
    "                     [df_traffic, df_accidents, df_weather, df_osm, df_transit]])\n",
    "print(f\"\\nüìä Total Records Analyzed: {total_records:,}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"   1. Review data quality findings above\")\n",
    "print(f\"   2. Continue to 02_Data_Cleaning.ipynb for preprocessing\")\n",
    "print(f\"   3. Handle missing values and outliers\")\n",
    "print(f\"   4. Engineer new features from raw data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Data Exploration Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
