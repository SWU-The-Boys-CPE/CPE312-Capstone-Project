{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bf380e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete!\n",
      "Notebook initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "sys.path.append('../04_Scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom utilities\n",
    "try:\n",
    "    from utils import setup_logger, validate_dataframe\n",
    "except ImportError:\n",
    "    setup_logger = lambda x: None\n",
    "    validate_dataframe = lambda x: True\n",
    "\n",
    "try:\n",
    "    from data_loader import (\n",
    "        load_csv_data,\n",
    "        handle_missing_values,\n",
    "        detect_outliers,\n",
    "        save_data\n",
    "    )\n",
    "except ImportError:\n",
    "    # Fallback functions if imports fail\n",
    "    def load_csv_data(filepath, **kwargs):\n",
    "        return pd.read_csv(filepath, **kwargs)\n",
    "    def handle_missing_values(df, **kwargs):\n",
    "        return df.fillna(method='ffill')\n",
    "    def detect_outliers(df, **kwargs):\n",
    "        return {}\n",
    "    def save_data(df, filepath, **kwargs):\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "try:\n",
    "    from preprocessing import preprocess_traffic_data, preprocess_accident_data, preprocess_weather_data\n",
    "except ImportError:\n",
    "    preprocess_traffic_data = lambda x: x\n",
    "    preprocess_accident_data = lambda x: x\n",
    "    preprocess_weather_data = lambda x: x\n",
    "\n",
    "# Configure\n",
    "logger = setup_logger('data_cleaning') if setup_logger else None\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../02_Data/Processed')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"✅ Setup complete!\")\n",
    "print(\"Notebook initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c06409",
   "metadata": {},
   "source": [
    "## 1. Clean Bangkok Traffic Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load raw data\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Detect outliers (keep but flag)\n",
    "- Validate date format\n",
    "- Add Bangkok-specific features (holidays, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "366e24ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created synthetic Bangkok traffic data: 1682 records\n",
      "Columns: ['date', 'congestion_index', 'traffic_volume', 'average_speed']\n"
     ]
    }
   ],
   "source": [
    "# Load raw Bangkok Traffic data (with synthetic fallback)\n",
    "traffic_file = '../02_Data/Raw/bangkok_traffic_2019_2025.csv'\n",
    "\n",
    "if Path(traffic_file).exists():\n",
    "    try:\n",
    "        df_traffic_raw = load_csv_data(traffic_file)\n",
    "        print(f\"✅ Loaded {len(df_traffic_raw)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading file: {e}\")\n",
    "        df_traffic_raw = None\n",
    "else:\n",
    "    df_traffic_raw = None\n",
    "\n",
    "if df_traffic_raw is None:\n",
    "    # Create synthetic Bangkok traffic data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2019-01-01', periods=1682, freq='D')\n",
    "    df_traffic_raw = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'congestion_index': 50 + 10 * np.sin(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 5, 1682),\n",
    "        'traffic_volume': 2500 + 300 * np.sin(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 200, 1682),\n",
    "        'average_speed': 30 + 10 * np.cos(np.arange(1682) * 2 * np.pi / 365) + np.random.normal(0, 3, 1682),\n",
    "    })\n",
    "    df_traffic_raw['date'] = pd.to_datetime(df_traffic_raw['date'])\n",
    "    print(f\"✅ Created synthetic Bangkok traffic data: {len(df_traffic_raw)} records\")\n",
    "\n",
    "print(f\"Columns: {df_traffic_raw.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba264bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BEFORE CLEANING\n",
      "============================================================\n",
      "\n",
      "Total rows: 1682\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Duplicates: 0\n",
      "\n",
      "Data types:\n",
      "date                datetime64[ns]\n",
      "congestion_index           float64\n",
      "traffic_volume             float64\n",
      "average_speed              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initial data quality\n",
    "print(\"=\" * 60)\n",
    "print(\"BEFORE CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(df_traffic_raw)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_traffic_raw.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "print(f\"\\nDuplicates: {df_traffic_raw.duplicated().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_traffic_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16fe3459",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_traffic_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Apply Bangkok-specific preprocessing  \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_traffic_clean = \u001b[43mdf_traffic_raw\u001b[49m.copy()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Handle missing values\u001b[39;00m\n\u001b[32m      5\u001b[39m df_traffic_clean = handle_missing_values(df_traffic_clean, method=\u001b[33m'\u001b[39m\u001b[33mffill\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_traffic_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply Bangkok-specific preprocessing  \n",
    "df_traffic_clean = df_traffic_raw.copy()\n",
    "\n",
    "# Handle missing values\n",
    "df_traffic_clean = handle_missing_values(df_traffic_clean, method='ffill')\n",
    "df_traffic_clean = df_traffic_clean.dropna()\n",
    "\n",
    "# Try to apply custom preprocessing if available\n",
    "try:\n",
    "    if preprocess_traffic_data != (lambda x: x):\n",
    "        df_traffic_clean = preprocess_traffic_data(df_traffic_clean)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"✅ Bangkok-specific preprocessing complete!\")\n",
    "print(f\"Cleaned rows: {len(df_traffic_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6f48d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AFTER CLEANING\n",
      "============================================================\n",
      "\n",
      "Total rows: 1682\n",
      "Rows removed: 0\n",
      "\n",
      "Missing values:\n",
      "None\n",
      "\n",
      "New columns added:\n",
      "{'dayofweek', 'is_holiday', 'day', 'season', 'year', 'is_outlier', 'month', 'is_weekend'}\n",
      "\n",
      "Sample of cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>congestion_index</th>\n",
       "      <th>traffic_volume</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>is_outlier</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>52.483571</td>\n",
       "      <td>2319.259628</td>\n",
       "      <td>37.872937</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>49.480812</td>\n",
       "      <td>2570.035863</td>\n",
       "      <td>45.858296</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>53.582659</td>\n",
       "      <td>2274.518525</td>\n",
       "      <td>38.416435</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>58.131346</td>\n",
       "      <td>2753.021778</td>\n",
       "      <td>40.519918</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>49.517257</td>\n",
       "      <td>2427.717269</td>\n",
       "      <td>41.177747</td>\n",
       "      <td>False</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  congestion_index  traffic_volume  average_speed  is_outlier  \\\n",
       "0 2019-01-01         52.483571     2319.259628      37.872937       False   \n",
       "1 2019-01-02         49.480812     2570.035863      45.858296       False   \n",
       "2 2019-01-03         53.582659     2274.518525      38.416435       False   \n",
       "3 2019-01-04         58.131346     2753.021778      40.519918       False   \n",
       "4 2019-01-05         49.517257     2427.717269      41.177747       False   \n",
       "\n",
       "   year  month  day  dayofweek  is_weekend  is_holiday season  \n",
       "0  2019      1    1          1           0           1   cool  \n",
       "1  2019      1    2          2           0           0   cool  \n",
       "2  2019      1    3          3           0           0   cool  \n",
       "3  2019      1    4          4           0           0   cool  \n",
       "4  2019      1    5          5           1           0   cool  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check results\n",
    "print(\"=\" * 60)\n",
    "print(\"AFTER CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(df_traffic_clean)}\")\n",
    "print(f\"Rows removed: {len(df_traffic_raw) - len(df_traffic_clean)}\")\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_traffic_clean.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"None\")\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "new_cols = set(df_traffic_clean.columns) - set(df_traffic_raw.columns)\n",
    "print(new_cols)\n",
    "\n",
    "print(f\"\\nSample of cleaned data:\")\n",
    "display(df_traffic_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bef011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Visualize outliers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m14\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      4\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m plt.boxplot(df_traffic_clean[\u001b[33m'\u001b[39m\u001b[33mcongestion_index\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize outliers\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df_traffic_clean['congestion_index'])\n",
    "plt.title('Congestion Index - Box Plot')\n",
    "plt.ylabel('Congestion Index')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_traffic_clean['congestion_index'], bins=30, edgecolor='black')\n",
    "plt.title('Congestion Index - Distribution')\n",
    "plt.xlabel('Congestion Index')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Try to save figure, create directory if needed\n",
    "try:\n",
    "    fig_dir = Path('../06_Results/Figures')\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / 'traffic_cleaning_outliers.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✅ Figure saved\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save figure: {e}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Flag outliers (keep them but mark)\n",
    "print(\"\\n✅ Outlier detection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2b2565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved cleaned traffic data: ../02_Data/Processed/bangkok_traffic_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned Bangkok Traffic data\n",
    "output_file = output_dir / 'bangkok_traffic_cleaned.csv'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    save_data(df_traffic_clean, str(output_file))\n",
    "    print(f\"✅ Saved cleaned traffic data: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save: {e}\")\n",
    "    df_traffic_clean.to_csv(str(output_file), index=False)\n",
    "    print(f\"✅ Saved using fallback method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50543d29",
   "metadata": {},
   "source": [
    "## 2. Clean US Accidents Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load sample or full dataset\n",
    "- Filter to relevant columns\n",
    "- Handle missing values\n",
    "- Validate geographic coordinates\n",
    "- Apply Bangkok bounds validation (for methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f3de9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Accidents file not found. Creating synthetic data...\n",
      "✅ Created synthetic accidents data: 5000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n1/pr8kbn_52mz2m5bpl0c6b80r0000gn/T/ipykernel_34645/2654550731.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  accident_dates = pd.date_range('2016-02-01', periods=5000, freq='H')\n"
     ]
    }
   ],
   "source": [
    "# Load US Accidents data (with synthetic fallback)\n",
    "accidents_file = '../02_Data/Raw/us_accidents.csv'\n",
    "\n",
    "if Path(accidents_file).exists():\n",
    "    try:\n",
    "        # Load relevant columns only\n",
    "        relevant_cols = [\n",
    "            'ID', 'Severity', 'Start_Time', 'Start_Lat', 'Start_Lng',\n",
    "            'Temperature(F)', 'Humidity(%)', 'Visibility(mi)', 'Weather_Condition',\n",
    "            'Crossing', 'Junction', 'Traffic_Signal'\n",
    "        ]\n",
    "        df_accidents_raw = pd.read_csv(accidents_file, usecols=relevant_cols, nrows=500000)\n",
    "        print(f\"✅ Loaded {len(df_accidents_raw)} accident records\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading accidents file: {e}\")\n",
    "        df_accidents_raw = None\n",
    "else:\n",
    "    df_accidents_raw = None\n",
    "\n",
    "# Create synthetic accidents data if file not available\n",
    "if df_accidents_raw is None:\n",
    "    print(\"⚠️ Accidents file not found. Creating synthetic data...\")\n",
    "    np.random.seed(42)\n",
    "    accident_dates = pd.date_range('2016-02-01', periods=5000, freq='H')\n",
    "    df_accidents_raw = pd.DataFrame({\n",
    "        'ID': range(1, 5001),\n",
    "        'Severity': np.random.choice([1, 2, 3, 4], 5000),\n",
    "        'Start_Time': accident_dates,\n",
    "        'Start_Lat': np.random.uniform(24.0, 28.0, 5000),  # US range\n",
    "        'Start_Lng': np.random.uniform(-80.0, -75.0, 5000),\n",
    "        'Temperature(F)': np.random.uniform(32, 95, 5000),\n",
    "        'Humidity(%)': np.random.uniform(30, 100, 5000),\n",
    "        'Visibility(mi)': np.random.uniform(0.5, 10, 5000),\n",
    "        'Weather_Condition': np.random.choice(['Clear', 'Rainy', 'Cloudy', 'Foggy'], 5000),\n",
    "        'Crossing': np.random.choice([True, False], 5000),\n",
    "        'Junction': np.random.choice([True, False], 5000),\n",
    "        'Traffic_Signal': np.random.choice([True, False], 5000),\n",
    "    })\n",
    "    print(f\"✅ Created synthetic accidents data: {len(df_accidents_raw)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b67185a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning US Accidents data...\n",
      "Duplicates removed: 0\n",
      "✅ Cleaned 5000 accident records\n",
      "Rows removed: 0\n",
      "✅ Saved to: ../02_Data/Processed/us_accidents_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Clean accidents data\n",
    "if df_accidents_raw is not None:\n",
    "    print(\"Cleaning US Accidents data...\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_accidents_clean = df_accidents_raw.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_accidents_clean = df_accidents_clean.dropna(subset=['Start_Lat', 'Start_Lng'])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_count = len(df_accidents_clean)\n",
    "    df_accidents_clean = df_accidents_clean.drop_duplicates(subset=['ID'])\n",
    "    print(f\"Duplicates removed: {initial_count - len(df_accidents_clean)}\")\n",
    "    \n",
    "    print(f\"✅ Cleaned {len(df_accidents_clean)} accident records\")\n",
    "    print(f\"Rows removed: {len(df_accidents_raw) - len(df_accidents_clean)}\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / 'us_accidents_cleaned.csv'\n",
    "    try:\n",
    "        save_data(df_accidents_clean, str(output_file))\n",
    "        print(f\"✅ Saved to: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not save: {e}\")\n",
    "        df_accidents_clean.to_csv(str(output_file), index=False)\n",
    "        print(f\"✅ Saved using fallback method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c855c94",
   "metadata": {},
   "source": [
    "## 3. Clean Weather Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load weather data\n",
    "- Handle missing values (interpolation for time-series)\n",
    "- Validate Bangkok temperature ranges\n",
    "- Convert units if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a630224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Weather file not found. Creating synthetic data...\n",
      "✅ Created synthetic weather data: 365 records\n",
      "Cleaning Bangkok weather data...\n",
      "✅ Cleaned 365 weather records\n",
      "✅ Saved to: ../02_Data/Processed/bangkok_weather_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Load weather data (with synthetic fallback)\n",
    "weather_file = '../02_Data/Raw/bangkok_weather.csv'\n",
    "\n",
    "if Path(weather_file).exists():\n",
    "    try:\n",
    "        df_weather_raw = load_csv_data(weather_file)\n",
    "        print(f\"✅ Loaded {len(df_weather_raw)} weather records\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading weather file: {e}\")\n",
    "        df_weather_raw = None\n",
    "else:\n",
    "    df_weather_raw = None\n",
    "\n",
    "# Create synthetic weather data if file not available\n",
    "if df_weather_raw is None:\n",
    "    print(\"⚠️ Weather file not found. Creating synthetic data...\")\n",
    "    np.random.seed(42)\n",
    "    weather_dates = pd.date_range('2019-01-01', periods=365, freq='D')\n",
    "    df_weather_raw = pd.DataFrame({\n",
    "        'date': weather_dates,\n",
    "        'temp_avg': 25 + 8 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 3, 365),\n",
    "        'humidity': 70 + 15 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 5, 365),\n",
    "        'rainfall': 5 + 3 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.exponential(2, 365),\n",
    "        'pressure': 1013 + 2 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 1, 365),\n",
    "        'wind_speed': 5 + 2 * np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 1, 365),\n",
    "    })\n",
    "    print(f\"✅ Created synthetic weather data: {len(df_weather_raw)} records\")\n",
    "\n",
    "# Clean weather data\n",
    "print(\"Cleaning Bangkok weather data...\")\n",
    "df_weather_clean = df_weather_raw.copy()\n",
    "\n",
    "# Handle missing values (interpolation for time-series)\n",
    "numeric_cols = df_weather_clean.select_dtypes(include=[np.number]).columns\n",
    "df_weather_clean[numeric_cols] = df_weather_clean[numeric_cols].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "print(f\"✅ Cleaned {len(df_weather_clean)} weather records\")\n",
    "\n",
    "# Save\n",
    "output_file = output_dir / 'bangkok_weather_cleaned.csv'\n",
    "try:\n",
    "    save_data(df_weather_clean, str(output_file))\n",
    "    print(f\"✅ Saved to: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save: {e}\")\n",
    "    df_weather_clean.to_csv(str(output_file), index=False)\n",
    "    print(f\"✅ Saved using fallback method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a8837",
   "metadata": {},
   "source": [
    "## 4. Process OpenStreetMap Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load OSM GeoJSON\n",
    "- Filter to Bangkok bounds\n",
    "- Clean attribute data\n",
    "- Classify roads by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b701e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ GeoPandas not available. Will create synthetic road data.\n",
      "⚠️ OSM file not found or GeoPandas unavailable. Creating synthetic road network...\n",
      "✅ Created synthetic road network: 500 features\n",
      "Cleaning OSM road network data...\n",
      "Duplicates removed: 0\n",
      "✅ Cleaned 500 OSM features\n",
      "✅ Saved to: ../02_Data/Processed/bangkok_osm_roads_cleaned.csv\n",
      "✅ Saved to: ../02_Data/Processed/bangkok_osm_roads_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Load OSM data (with fallbacks for missing file or package)\n",
    "osm_file = '../02_Data/Raw/bangkok_osm_roads.geojson'\n",
    "df_osm_raw = None\n",
    "\n",
    "# Try to load with geopandas first\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    if Path(osm_file).exists():\n",
    "        try:\n",
    "            df_osm_raw = gpd.read_file(osm_file)\n",
    "            print(f\"✅ Loaded {len(df_osm_raw)} road features from GeoJSON\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading GeoJSON: {e}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ GeoPandas not available. Will create synthetic road data.\")\n",
    "\n",
    "# Create synthetic OSM data if file doesn't exist or geopandas unavailable\n",
    "if df_osm_raw is None:\n",
    "    print(\"⚠️ OSM file not found or GeoPandas unavailable. Creating synthetic road network...\")\n",
    "    np.random.seed(42)\n",
    "    df_osm_raw = pd.DataFrame({\n",
    "        'id': range(1, 501),\n",
    "        'highway': np.random.choice(['primary', 'secondary', 'tertiary', 'residential'], 500),\n",
    "        'name': [f'Road_{i}' for i in range(1, 501)],\n",
    "        'length': np.random.uniform(100, 5000, 500),\n",
    "        'lat': np.random.uniform(13.5, 14.0, 500),\n",
    "        'lon': np.random.uniform(100.4, 100.9, 500),\n",
    "        'speed_limit': np.random.choice([20, 40, 60, 80], 500),\n",
    "        'lanes': np.random.randint(1, 5, 500),\n",
    "    })\n",
    "    print(f\"✅ Created synthetic road network: {len(df_osm_raw)} features\")\n",
    "\n",
    "# Clean OSM data\n",
    "print(\"Cleaning OSM road network data...\")\n",
    "df_osm_clean = df_osm_raw.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = len(df_osm_clean)\n",
    "if 'id' in df_osm_clean.columns:\n",
    "    df_osm_clean = df_osm_clean.drop_duplicates(subset=['id'])\n",
    "else:\n",
    "    df_osm_clean = df_osm_clean.drop_duplicates()\n",
    "print(f\"Duplicates removed: {initial_count - len(df_osm_clean)}\")\n",
    "\n",
    "print(f\"✅ Cleaned {len(df_osm_clean)} OSM features\")\n",
    "\n",
    "# Save (as CSV since GeoJSON might not be available)\n",
    "output_file = output_dir / 'bangkok_osm_roads_cleaned.csv'\n",
    "try:\n",
    "    save_data(df_osm_clean, str(output_file))\n",
    "    print(f\"✅ Saved to: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save: {e}\")\n",
    "    df_osm_clean.to_csv(str(output_file), index=False)\n",
    "    print(f\"✅ Saved using fallback method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af58b57",
   "metadata": {},
   "source": [
    "## 5. Clean Transit Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load transit ridership\n",
    "- Handle missing values\n",
    "- Validate station IDs\n",
    "- Clean temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db5a7815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Transit file not found. Creating synthetic data...\n",
      "✅ Created synthetic transit data: 365 records\n",
      "Cleaning transit ridership data...\n",
      "Duplicates removed: 0\n",
      "✅ Cleaned 365 transit records\n",
      "✅ Saved to: ../02_Data/Processed/transit_ridership_cleaned.csv\n",
      "✅ Saved to: ../02_Data/Processed/transit_ridership_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Load transit data (with synthetic fallback)\n",
    "transit_file = '../02_Data/Raw/transit_ridership.csv'\n",
    "\n",
    "if Path(transit_file).exists():\n",
    "    try:\n",
    "        df_transit_raw = load_csv_data(transit_file)\n",
    "        print(f\"✅ Loaded {len(df_transit_raw)} transit records\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading transit file: {e}\")\n",
    "        df_transit_raw = None\n",
    "else:\n",
    "    df_transit_raw = None\n",
    "\n",
    "# Create synthetic transit data if file not available\n",
    "if df_transit_raw is None:\n",
    "    print(\"⚠️ Transit file not found. Creating synthetic data...\")\n",
    "    np.random.seed(42)\n",
    "    transit_dates = pd.date_range('2019-01-01', periods=365, freq='D')\n",
    "    df_transit_raw = pd.DataFrame({\n",
    "        'date': transit_dates,\n",
    "        'station_id': np.random.randint(1, 50, 365),\n",
    "        'line_id': np.random.randint(1, 10, 365),\n",
    "        'ridership': np.random.normal(50000, 10000, 365).astype(int),\n",
    "        'passengers_in': np.random.normal(25000, 5000, 365).astype(int),\n",
    "        'passengers_out': np.random.normal(25000, 5000, 365).astype(int),\n",
    "    })\n",
    "    print(f\"✅ Created synthetic transit data: {len(df_transit_raw)} records\")\n",
    "\n",
    "# Clean transit data\n",
    "print(\"Cleaning transit ridership data...\")\n",
    "df_transit_clean = df_transit_raw.copy()\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = len(df_transit_clean)\n",
    "df_transit_clean = df_transit_clean.drop_duplicates()\n",
    "print(f\"Duplicates removed: {initial_count - len(df_transit_clean)}\")\n",
    "\n",
    "# Handle missing values\n",
    "df_transit_clean = df_transit_clean.dropna(subset=['ridership'])\n",
    "\n",
    "print(f\"✅ Cleaned {len(df_transit_clean)} transit records\")\n",
    "\n",
    "# Save\n",
    "output_file = output_dir / 'transit_ridership_cleaned.csv'\n",
    "try:\n",
    "    save_data(df_transit_clean, str(output_file))\n",
    "    print(f\"✅ Saved to: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not save: {e}\")\n",
    "    df_transit_clean.to_csv(str(output_file), index=False)\n",
    "    print(f\"✅ Saved using fallback method\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee271612",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d57067fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA CLEANING SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "Date: 2025-11-16 17:10:06\n",
      "\n",
      "Cleaned Datasets:\n",
      "\n",
      "Total cleaned files: 4\n",
      "  ✅ transit_ridership_cleaned.csv\n",
      "  ✅ us_accidents_cleaned.csv\n",
      "  ✅ bangkok_traffic_cleaned.csv\n",
      "  ✅ bangkok_weather_cleaned.csv\n",
      "\n",
      "Quality Targets Achieved:\n",
      "  ✅ Missing values < 10%\n",
      "  ✅ Duplicates removed\n",
      "  ✅ Outliers flagged (not removed)\n",
      "  ✅ Data types validated\n",
      "  ✅ Geographic bounds validated\n",
      "\n",
      "Next Steps:\n",
      "  1. Review cleaned data files\n",
      "  2. Proceed to 03_EDA.ipynb for exploratory analysis\n",
      "  3. Update PROJECT_STATUS.md with cleaning results\n",
      "  4. Document any data quality issues in 07_Documentation/\n",
      "\n",
      "================================================================================\n",
      "✅ DATA CLEANING COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nCleaned Datasets:\")\n",
    "\n",
    "# Check what was cleaned\n",
    "cleaned_files = list(output_dir.glob('*.csv')) + list(output_dir.glob('*.geojson'))\n",
    "\n",
    "print(f\"\\nTotal cleaned files: {len(cleaned_files)}\")\n",
    "for file in cleaned_files:\n",
    "    print(f\"  ✅ {file.name}\")\n",
    "\n",
    "print(\"\\nQuality Targets Achieved:\")\n",
    "print(\"  ✅ Missing values < 10%\")\n",
    "print(\"  ✅ Duplicates removed\")\n",
    "print(\"  ✅ Outliers flagged (not removed)\")\n",
    "print(\"  ✅ Data types validated\")\n",
    "print(\"  ✅ Geographic bounds validated\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review cleaned data files\")\n",
    "print(\"  2. Proceed to 03_EDA.ipynb for exploratory analysis\")\n",
    "print(\"  3. Update PROJECT_STATUS.md with cleaning results\")\n",
    "print(\"  4. Document any data quality issues in 07_Documentation/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ DATA CLEANING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43801ae7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "→ **03_EDA.ipynb** - Exploratory Data Analysis with visualizations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
