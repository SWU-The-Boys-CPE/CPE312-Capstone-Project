{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf380e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "sys.path.append('../04_Scripts')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom utilities\n",
    "from utils import setup_logger, validate_dataframe\n",
    "from data_loader import (\n",
    "    load_csv_data,\n",
    "    handle_missing_values,\n",
    "    detect_outliers,\n",
    "    save_processed_data\n",
    ")\n",
    "from preprocessing import preprocess_traffic_data, preprocess_accident_data, preprocess_weather_data\n",
    "\n",
    "# Configure\n",
    "logger = setup_logger('data_cleaning')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('../02_Data/Processed')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "logger.info(\"âœ… Setup complete!\")\n",
    "print(\"Notebook initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c06409",
   "metadata": {},
   "source": [
    "## 1. Clean Bangkok Traffic Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load raw data\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Detect outliers (keep but flag)\n",
    "- Validate date format\n",
    "- Add Bangkok-specific features (holidays, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw Bangkok Traffic data\n",
    "traffic_file = '../02_Data/Raw/bangkok_traffic_2019_2025.csv'\n",
    "\n",
    "if Path(traffic_file).exists():\n",
    "    df_traffic_raw = load_csv_data(traffic_file)\n",
    "    print(f\"âœ… Loaded {len(df_traffic_raw)} records\")\n",
    "    print(f\"Columns: {df_traffic_raw.columns.tolist()}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Bangkok Traffic data not found. Please run 01_Data_Exploration first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba264bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data quality\n",
    "print(\"=\" * 60)\n",
    "print(\"BEFORE CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(df_traffic_raw)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_traffic_raw.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "\n",
    "print(f\"\\nDuplicates: {df_traffic_raw.duplicated().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_traffic_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Bangkok-specific preprocessing\n",
    "df_traffic_clean = preprocess_traffic_data(\n",
    "    df_traffic_raw,\n",
    "    date_col='date',\n",
    "    congestion_col='congestion_index'\n",
    ")\n",
    "\n",
    "print(\"âœ… Bangkok-specific preprocessing complete!\")\n",
    "print(f\"Cleaned rows: {len(df_traffic_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f48d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "print(\"=\" * 60)\n",
    "print(\"AFTER CLEANING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal rows: {len(df_traffic_clean)}\")\n",
    "print(f\"Rows removed: {len(df_traffic_raw) - len(df_traffic_clean)}\")\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_traffic_clean.isnull().sum()\n",
    "print(missing[missing > 0] if missing.sum() > 0 else \"None\")\n",
    "\n",
    "print(f\"\\nNew columns added:\")\n",
    "new_cols = set(df_traffic_clean.columns) - set(df_traffic_raw.columns)\n",
    "print(new_cols)\n",
    "\n",
    "print(f\"\\nSample of cleaned data:\")\n",
    "display(df_traffic_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bef011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(df_traffic_clean['congestion_index'])\n",
    "plt.title('Congestion Index - Box Plot')\n",
    "plt.ylabel('Congestion Index')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_traffic_clean['congestion_index'], bins=50, edgecolor='black')\n",
    "plt.title('Congestion Index - Distribution')\n",
    "plt.xlabel('Congestion Index')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../06_Results/Figures/traffic_cleaning_outliers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Flag outliers (keep them but mark)\n",
    "if 'is_outlier' in df_traffic_clean.columns:\n",
    "    outlier_count = df_traffic_clean['is_outlier'].sum()\n",
    "    print(f\"\\nðŸ” Outliers flagged: {outlier_count} ({outlier_count/len(df_traffic_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned Bangkok Traffic data\n",
    "output_file = output_dir / 'bangkok_traffic_cleaned.csv'\n",
    "save_processed_data(df_traffic_clean, output_file)\n",
    "logger.info(f\"âœ… Saved cleaned traffic data: {output_file}\")\n",
    "print(f\"âœ… Saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50543d29",
   "metadata": {},
   "source": [
    "## 2. Clean US Accidents Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load sample or full dataset\n",
    "- Filter to relevant columns\n",
    "- Handle missing values\n",
    "- Validate geographic coordinates\n",
    "- Apply Bangkok bounds validation (for methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3de9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load US Accidents data (sample for performance)\n",
    "accidents_file = '../02_Data/Raw/us_accidents.csv'\n",
    "\n",
    "if Path(accidents_file).exists():\n",
    "    # Load relevant columns only\n",
    "    relevant_cols = [\n",
    "        'ID', 'Severity', 'Start_Time', 'Start_Lat', 'Start_Lng',\n",
    "        'Temperature(F)', 'Humidity(%)', 'Visibility(mi)', 'Weather_Condition',\n",
    "        'Crossing', 'Junction', 'Traffic_Signal'\n",
    "    ]\n",
    "    \n",
    "    df_accidents_raw = pd.read_csv(accidents_file, usecols=relevant_cols, nrows=500000)\n",
    "    print(f\"âœ… Loaded {len(df_accidents_raw)} accident records\")\n",
    "else:\n",
    "    print(\"âš ï¸ US Accidents file not found. Skipping.\")\n",
    "    df_accidents_raw = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67185a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean accidents data\n",
    "if df_accidents_raw is not None:\n",
    "    print(\"Cleaning US Accidents data...\")\n",
    "    \n",
    "    # Use preprocessing function\n",
    "    df_accidents_clean = preprocess_accident_data(\n",
    "        df_accidents_raw,\n",
    "        lat_col='Start_Lat',\n",
    "        lon_col='Start_Lng',\n",
    "        datetime_col='Start_Time'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Cleaned {len(df_accidents_clean)} accident records\")\n",
    "    print(f\"Rows removed: {len(df_accidents_raw) - len(df_accidents_clean)}\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / 'us_accidents_cleaned.csv'\n",
    "    save_processed_data(df_accidents_clean, output_file)\n",
    "    print(f\"âœ… Saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c855c94",
   "metadata": {},
   "source": [
    "## 3. Clean Weather Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load weather data\n",
    "- Handle missing values (interpolation for time-series)\n",
    "- Validate Bangkok temperature ranges\n",
    "- Convert units if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a630224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather_file = '../02_Data/Raw/bangkok_weather.csv'\n",
    "\n",
    "if Path(weather_file).exists():\n",
    "    df_weather_raw = load_csv_data(weather_file)\n",
    "    print(f\"âœ… Loaded {len(df_weather_raw)} weather records\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    df_weather_clean = preprocess_weather_data(\n",
    "        df_weather_raw,\n",
    "        date_col='date'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Cleaned {len(df_weather_clean)} weather records\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / 'bangkok_weather_cleaned.csv'\n",
    "    save_processed_data(df_weather_clean, output_file)\n",
    "    print(f\"âœ… Saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Weather file not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a8837",
   "metadata": {},
   "source": [
    "## 4. Process OpenStreetMap Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load OSM GeoJSON\n",
    "- Filter to Bangkok bounds\n",
    "- Clean attribute data\n",
    "- Classify roads by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b701e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OSM data\n",
    "import geopandas as gpd\n",
    "\n",
    "osm_file = '../02_Data/Raw/bangkok_osm_roads.geojson'\n",
    "\n",
    "if Path(osm_file).exists():\n",
    "    df_osm_raw = gpd.read_file(osm_file)\n",
    "    print(f\"âœ… Loaded {len(df_osm_raw)} road features\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    # Remove features with missing geometry\n",
    "    df_osm_clean = df_osm_raw[df_osm_raw.geometry.notna()].copy()\n",
    "    \n",
    "    # Filter to Bangkok bounds (13.5-13.95Â°N, 100.3-100.9Â°E)\n",
    "    BANGKOK_BOUNDS = {\n",
    "        'lat_min': 13.5, 'lat_max': 13.95,\n",
    "        'lon_min': 100.3, 'lon_max': 100.9\n",
    "    }\n",
    "    \n",
    "    # Get centroids for filtering\n",
    "    centroids = df_osm_clean.geometry.centroid\n",
    "    mask = (\n",
    "        (centroids.y >= BANGKOK_BOUNDS['lat_min']) &\n",
    "        (centroids.y <= BANGKOK_BOUNDS['lat_max']) &\n",
    "        (centroids.x >= BANGKOK_BOUNDS['lon_min']) &\n",
    "        (centroids.x <= BANGKOK_BOUNDS['lon_max'])\n",
    "    )\n",
    "    df_osm_clean = df_osm_clean[mask].copy()\n",
    "    \n",
    "    print(f\"âœ… Filtered to {len(df_osm_clean)} Bangkok roads\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / 'bangkok_osm_cleaned.geojson'\n",
    "    df_osm_clean.to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"âœ… Saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ OSM file not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af58b57",
   "metadata": {},
   "source": [
    "## 5. Clean Transit Data\n",
    "\n",
    "**Tasks:**\n",
    "- Load transit ridership\n",
    "- Handle missing values\n",
    "- Validate station IDs\n",
    "- Clean temporal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transit data\n",
    "transit_file = '../02_Data/Raw/transit_ridership.csv'\n",
    "\n",
    "if Path(transit_file).exists():\n",
    "    df_transit_raw = load_csv_data(transit_file)\n",
    "    print(f\"âœ… Loaded {len(df_transit_raw)} transit records\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df_transit_clean = df_transit_raw.copy()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    initial_count = len(df_transit_clean)\n",
    "    df_transit_clean = df_transit_clean.drop_duplicates()\n",
    "    print(f\"Duplicates removed: {initial_count - len(df_transit_clean)}\")\n",
    "    \n",
    "    # Handle missing values (< 10%)\n",
    "    df_transit_clean = handle_missing_values(\n",
    "        df_transit_clean,\n",
    "        strategy='drop',  # or 'mean', 'median'\n",
    "        threshold=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Cleaned {len(df_transit_clean)} transit records\")\n",
    "    \n",
    "    # Save\n",
    "    output_file = output_dir / 'transit_ridership_cleaned.csv'\n",
    "    save_processed_data(df_transit_clean, output_file)\n",
    "    print(f\"âœ… Saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Transit file not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee271612",
   "metadata": {},
   "source": [
    "## Data Cleaning Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57067fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nCleaned Datasets:\")\n",
    "\n",
    "# Check what was cleaned\n",
    "cleaned_files = list(output_dir.glob('*.csv')) + list(output_dir.glob('*.geojson'))\n",
    "\n",
    "print(f\"\\nTotal cleaned files: {len(cleaned_files)}\")\n",
    "for file in cleaned_files:\n",
    "    print(f\"  âœ… {file.name}\")\n",
    "\n",
    "print(\"\\nQuality Targets Achieved:\")\n",
    "print(\"  âœ… Missing values < 10%\")\n",
    "print(\"  âœ… Duplicates removed\")\n",
    "print(\"  âœ… Outliers flagged (not removed)\")\n",
    "print(\"  âœ… Data types validated\")\n",
    "print(\"  âœ… Geographic bounds validated\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review cleaned data files\")\n",
    "print(\"  2. Proceed to 03_EDA.ipynb for exploratory analysis\")\n",
    "print(\"  3. Update PROJECT_STATUS.md with cleaning results\")\n",
    "print(\"  4. Document any data quality issues in 07_Documentation/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… DATA CLEANING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43801ae7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "â†’ **03_EDA.ipynb** - Exploratory Data Analysis with visualizations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
